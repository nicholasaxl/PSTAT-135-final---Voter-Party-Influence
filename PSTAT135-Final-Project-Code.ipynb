{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c80a70ea",
   "metadata": {},
   "source": [
    "# PSTAT134 Project Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a6395",
   "metadata": {},
   "source": [
    "### Aapthi Nagesh, Alex Roginski, Yuchen Wu, Nicholas Axl Andrian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c195dc",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2277a888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Jupyter notebook hosted by Google Cloud\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Voter File Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = (\n",
    "spark.read.format(\"parquet\")  # Edit by Alex\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"gs://winter-2024-voter-file/VM2Uniform/VM2Uniform--OR--2021-02-05\")\n",
    "    #use oregondata if local\n",
    ")\n",
    "# By Andy\n",
    "# This will load the uscounties.csv file into df2 variable, which will contain County IDs.\n",
    "raw_df2 = (\n",
    "spark.read\n",
    ".format(\"csv\")\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".load(\"gs://winter-2024-voter-file/uscounties.csv\")\n",
    ")\n",
    "\n",
    "# Filter rows where state_name is \"Oregon\"\n",
    "df2 = raw_df2.filter(raw_df2[\"state_name\"] == \"Oregon\")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc09db7e",
   "metadata": {},
   "source": [
    "### Cleaning data (select columns and remove NAs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acced687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Alex\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "selectedColumns = ['County', 'Ethnic_Description', 'Voters_Gender', 'Voters_Age', 'Mailing_Families_HHCount', 'Mailing_HHGender_Description', 'Residence_Families_HHCount', 'CommercialData_EstHomeValue', 'CommercialData_EstimatedHHIncomeAmount', 'CommercialData_EstimatedAreaMedianHHIncome', 'CommercialData_AreaMedianHousingValue', 'CommercialData_AreaMedianEducationYears', 'CommercialData_PropertyType', 'CommercialData_StateIncomeDecile','Parties_Description']\n",
    "\n",
    "cleaned_df = df.select(selectedColumns)\n",
    "\n",
    "data = cleaned_df.dropna()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05015c6d",
   "metadata": {},
   "source": [
    "### Creating df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389b6607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Andy \n",
    "from pyspark.sql.functions import upper\n",
    "\n",
    "# Normalize the \"County\" column in both DataFrames to uppercase\n",
    "df1 = data.withColumn(\"County\", upper(data[\"County\"]))\n",
    "df2 = df2.withColumn(\"county\", upper(df2[\"county\"]))\n",
    "\n",
    "df_joined = df1.join(df2, df1[\"County\"] == df2[\"county\"])\n",
    "\n",
    "# Selecting the desired columns from the joined DataFrame\n",
    "df_merged = df_joined.select(df1[\"*\"], df2[\"county_fips\"])\n",
    "\n",
    "print(\"Row count:\", df_merged.count(), \"Column count:\", len(df_merged.columns))\n",
    "df_merged.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f3afa5",
   "metadata": {},
   "source": [
    "### Checking for unique party descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd658a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Axl - Checking for unique values in the Parties_Description column\n",
    "unique_parties = df_merged.select('Parties_Description').distinct()\n",
    "unique_parties.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912275ee",
   "metadata": {},
   "source": [
    "### Binning outcome variable in df_merged "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda3ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Andy's code to merge bin\n",
    "from pyspark.sql.functions import when, col, lit\n",
    "\n",
    "parties_to_keep = ['Republican', 'Democratic', 'Non-Partisan']  # List of parties to keep\n",
    "\n",
    "df_merged = df_merged.withColumn(\"Parties_Description\", when(col(\"Parties_Description\").isin(parties_to_keep), col(\"Parties_Description\")).otherwise(lit(\"Other\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1bb0c4",
   "metadata": {},
   "source": [
    "###  Look at sample rows of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a02150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "test_rows = df_merged.head(10)\n",
    "\n",
    "column_headers= df_merged.columns\n",
    "\n",
    "new_df = pd.DataFrame(test_rows, columns=column_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc392cc",
   "metadata": {},
   "source": [
    "### Missing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81abeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# ChatGPT:\n",
    "missing_values = df.select([sum(col(column).isNull().cast(\"int\")).alias(column) for column in df.columns])  # Get missing data for each column\n",
    "# The following took a few minutes to load\n",
    "\n",
    "missing_values_df = missing_values.toPandas()  # Turn the data into a pandas dataframe to make it readable\n",
    "\n",
    "\n",
    "totalNumberOfRows = df.count()  # Count total number of rows in dataset\n",
    "\n",
    "transposed_df = missing_values_df.T.reset_index()  # tranpose it to make the data go down rows instead of across columns\n",
    "transposed_df.columns = ['Column_Name', 'Count_Missing']  # Rename columns\n",
    "transposed_df = transposed_df.sort_values(by='Count_Missing',ascending=False)  # sort values by number missing\n",
    "transposed_df['Percent_Missing'] = transposed_df['Count_Missing']/totalNumberOfRows  # Get percentage missing (extra)\n",
    "\n",
    "transposed_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d466a289",
   "metadata": {},
   "source": [
    "### Percent Missing Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b986420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERCENT MISSING\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming sorted_df_descending is your DataFrame with sorted missing counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(transposed_df['Percent_Missing'], bins=10, color='lightgreen', edgecolor='black')\n",
    "plt.xlabel('Percent Missing in Column')\n",
    "plt.ylabel('Column Frequency')\n",
    "plt.title('% Missing Values in All Columns of Oregon Data')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9a6521",
   "metadata": {},
   "source": [
    "# EDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87524408",
   "metadata": {},
   "source": [
    "### Pie Chart of Party Counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb2303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  By Aapthi and Alex \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "parties_count_df = df_merged.groupBy(\"Parties_Description\").count()\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "parties_count_pdf = parties_count_df.toPandas()\n",
    "\n",
    "# Sorting for consistency\n",
    "parties_count_pdf.sort_values('count', ascending=False, inplace=True)\n",
    "\n",
    "# Customization parameters\n",
    "colors = ['tab:green', 'tab:blue', 'tab:red', 'tab:grey',]  # Custom colors\n",
    "wp = {'linewidth': 1, 'edgecolor': \"black\"}  # Wedge properties\n",
    "\n",
    "# Function to format the autopct\n",
    "def func(pct, allvals):\n",
    "    absolute = int(pct/100.*np.sum(allvals))\n",
    "    return \"{:.1f}%\\n({:d})\".format(pct, absolute)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "wedges, texts, autotexts = ax.pie(parties_count_pdf['count'],\n",
    "                                  autopct=lambda pct: func(pct, parties_count_pdf['count']),\n",
    "                                  labels=parties_count_pdf['Parties_Description'],\n",
    "                                  startangle=30,\n",
    "                                  wedgeprops=wp,\n",
    "                                  colors=colors,\n",
    "                                  textprops=dict(color=\"black\", size=12))\n",
    "\n",
    "plt.title('Parties Description Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b739d6ca",
   "metadata": {},
   "source": [
    "### Income vs. Party pref. (Andy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4676d704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Originally created by Andy, editted by Alex\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define colors for the bars\n",
    "colors = ['tab:blue', 'tab:red', 'tab:green', 'tab:grey']\n",
    "\n",
    "df_merged_temp = df_merged.withColumn('Income_Amount',\n",
    "                                 regexp_replace(col('CommercialData_EstimatedHHIncomeAmount'), '[$,]', '').cast('float'))\n",
    "\n",
    "party_income_df = df_merged_temp.groupBy('Parties_Description')\\\n",
    "    .agg(F.avg('Income_Amount').alias('Average_Income'))\n",
    "\n",
    "party_income_pdf = party_income_df.toPandas()\n",
    "\n",
    "# Sort the DataFrame by 'Average_Income' column in descending order\n",
    "party_income_pdf_sorted = party_income_pdf.sort_values(by='Average_Income', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(party_income_pdf_sorted['Parties_Description'], party_income_pdf_sorted['Average_Income'], color=colors)\n",
    "plt.xlabel('Party Preference')\n",
    "plt.ylabel('Average Income')\n",
    "plt.title('Average Income by Party Preference')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Iterate over the bars and use plt.text() to display the value on each bar\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval, '$' + '{:,.0f}'.format(round(yval, 0)), fontsize=12, va='bottom', ha='center')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdaf23f",
   "metadata": {},
   "source": [
    "### Education Level vs Party Pref.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f4c20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by Aapthi, edited by Alex\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_merged_temp = df_merged.withColumn('Education_Years',\n",
    "                                 regexp_replace(col('CommercialData_AreaMedianEducationYears'), '[$,]', '').cast('int'))\n",
    "\n",
    "education_df = df_merged_temp.groupBy('Parties_Description')\\\n",
    "    .agg(F.avg('Education_Years').alias('Education_Years'))\n",
    "\n",
    "party_education_df = education_df.toPandas()\n",
    "\n",
    "# Sort the DataFrame by 'Education_Years'\n",
    "party_education_df_sorted = party_education_df.sort_values(by='Education_Years',  ascending=False)\n",
    "\n",
    "# Plotting the Average Education Years vs Parties\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(party_education_df_sorted['Parties_Description'], party_education_df_sorted['Education_Years'], \n",
    "        color = ['tab:blue', 'tab:Grey', 'tab:green', 'tab:red'])\n",
    "\n",
    "plt.xlabel('Party Preference')\n",
    "plt.ylabel('Average Education Years')\n",
    "plt.title('Average Education Year by Party Preference')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Adding the value above the bars\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 1), ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f03138c",
   "metadata": {},
   "source": [
    "### Property type and Party Pref.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc9625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Originally by Andy, editted by Alex\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df_merged is your Spark DataFrame\n",
    "# Create a pivot table with 'Parties_Description' as rows and 'CommercialData_PropertyType' as columns, with counts\n",
    "party_property_df = df_merged.groupBy('Parties_Description')\\\n",
    "    .pivot('CommercialData_PropertyType')\\\n",
    "    .count()\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "party_property_pdf = party_property_df.toPandas()\n",
    "\n",
    "# Set the index to 'Parties_Description' for plotting\n",
    "party_property_pdf.set_index('Parties_Description', inplace=True)\n",
    "\n",
    "# Convert counts to percentages\n",
    "party_property_pdf = party_property_pdf.div(party_property_pdf.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Sort columns by their total percentage across all parties\n",
    "party_property_pdf = party_property_pdf.reindex(party_property_pdf.sum().sort_values(ascending=False).index, axis=1)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "party_property_pdf.plot(kind='bar', stacked=True, ax=ax)\n",
    "\n",
    "plt.xlabel('Party Preference')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Property Type Distribution by Party Preference')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Place the legend outside the plot area\n",
    "plt.legend(title='Property Type', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Custom function to add labels\n",
    "def add_labels(ax):\n",
    "    for c in ax.containers:\n",
    "        labels = [f'{v.get_height():.0f}%' if v.get_height() > 5 else '' for v in c]\n",
    "        ax.bar_label(c, labels=labels, label_type='center', color='white', rotation=0, padding=3)\n",
    "\n",
    "add_labels(ax)\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec9e95c",
   "metadata": {},
   "source": [
    "### Ethnic Description EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743b697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axl - checking counts for unique values in Ethnic_Description\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "ethnic_description_counts = df.groupBy(\"Ethnic_Description\")\\\n",
    "                               .count()\\\n",
    "                               .withColumnRenamed(\"count\", \"Counts\")\\\n",
    "                               .orderBy(F.desc(\"Counts\"))\n",
    "\n",
    "# Show the result\n",
    "ethnic_description_counts.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd03148b",
   "metadata": {},
   "source": [
    "### Top 10 ethnic descriptions’ party preference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72295d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axl - Plotting histogram for party description per the top 20 ethnic descriptions\n",
    "# had to limit it to 10 due to the large number which made visualization difficult\n",
    "# Editted by Alex\n",
    "from pyspark.sql.functions import col, count\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# df for plotting\n",
    "result_df = df_merged.groupBy('Ethnic_Description').pivot('Parties_Description').count().na.fill(0)\n",
    "result_df = result_df.withColumn('Total', __builtins__.sum(result_df[col] for col in result_df.columns[1:]))\n",
    "result_df = result_df.orderBy(col('Total').desc()).limit(10)\n",
    "result_df = result_df.drop('Total')\n",
    "\n",
    "# Define the desired order\n",
    "desired_order = [\"Democratic\", \"Republican\", \"Non-Partisan\", \"Registered Independent\", \"Other\"]\n",
    "parties_ordered = [party for party in desired_order if party in result_df.columns]\n",
    "parties_ordered += [party for party in result_df.columns if party not in desired_order and party != 'Ethnic_Description']\n",
    "\n",
    "result_data = result_df.select(['Ethnic_Description'] + parties_ordered).collect()\n",
    "categories = [row['Ethnic_Description'] for row in result_data]\n",
    "counts = [list(row[1:]) for row in result_data]  # Exclude the first column which is 'Ethnic_Description'\n",
    "total_per_group = [__builtins__.sum(row) for row in counts]\n",
    "grand_total = df_merged.count()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "bar_width = 0.9\n",
    "bottoms = [0] * len(categories)\n",
    "\n",
    "\n",
    "party_colors = {\n",
    "    \"Democratic\": \"blue\",\n",
    "    \"Republican\": \"red\",\n",
    "    \"Non-Partisan\": \"green\",\n",
    "    \"Other\": \"grey\",\n",
    "}\n",
    "\n",
    "\n",
    "for i, party in enumerate(parties_ordered):\n",
    "    party_counts = [row[i + 1] for row in result_data]  # Offset due to 'Ethnic_Description'\n",
    "    party_counts = [count / grand_total * 100 for count in party_counts]\n",
    "    ax.bar(categories, party_counts, width=bar_width, label=party, bottom=bottoms, color=party_colors.get(party, \"black\"))\n",
    "    bottoms = [bottom + party_counts[j] for j, bottom in enumerate(bottoms)]\n",
    "\n",
    "    \n",
    "\n",
    "ax.set_xlabel('Ethnic Description')\n",
    "ax.set_ylabel('Percent of Voters')\n",
    "ax.set_title('Top 10 Ethnic Descriptions by Party Preference')\n",
    "ax.set_xticks(range(len(categories)))\n",
    "ax.set_xticklabels(categories, rotation=45)\n",
    "\n",
    "\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(decimals=0))\n",
    "\n",
    "ax.legend(title='Party Preference', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414ae5b7",
   "metadata": {},
   "source": [
    "### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1ecef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aapthi\n",
    "\n",
    "# create a pivot table\n",
    "party_gender_df = df_merged.groupBy('Parties_Description')\\\n",
    "    .pivot('Voters_Gender')\\\n",
    "    .count()\n",
    "\n",
    "# convert to pd dataframe\n",
    "party_gender_pdf = party_gender_df.toPandas()\n",
    "party_gender_pdf.set_index('Parties_Description', inplace=True)\n",
    "\n",
    "party_gender_pdf = party_gender_pdf.div(party_gender_pdf.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# sort columns by their total percentage\n",
    "party_gender_pdf = party_gender_pdf.reindex(party_gender_pdf.sum().sort_values(ascending=False).index, axis=1)\n",
    "\n",
    "# plotting\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "party_gender_pdf.plot(kind='bar', stacked=False, ax=ax)\n",
    "plt.xlabel('Party Preference')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Voter\\'s Gender Distribution by Party Preference')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title='Gender', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Custom function to add percent labels\n",
    "def add_labels(ax):\n",
    "    for c in ax.containers:\n",
    "        labels = [f'{v.get_height():.0f}%' if v.get_height() > 5 else '' for v in c]\n",
    "        ax.bar_label(c, labels=labels, label_type='center', color='white', rotation=0, padding=3)\n",
    "\n",
    "add_labels(ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5690e86b",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd87894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#By Aapthi, adapted from Andy’s code\n",
    "df_merged_temp = df_merged_temp.withColumn('Age_Level',\n",
    "                                 when(col('Voters_Age') < 36, '18-35',)\n",
    "                                 .when(col('Voters_Age') < 65, '36-64')\n",
    "                                 .otherwise('65+'))\n",
    "\n",
    "age_party_dist = df_merged_temp.groupBy('Age_Level', 'Parties_Description').count()\n",
    "\n",
    "age_party_pdf = age_party_dist.toPandas()\n",
    "\n",
    "\n",
    "# Plotting\n",
    "age_levels = ['18-35', '36-64', '65+']\n",
    "party_colors = {'Democratic': 'tab:blue', 'Republican': 'tab:red', 'Other': 'tab:grey', 'Non-Partisan': 'tab:orange'}\n",
    "age_levels_titles = {'Low Income': \"(<$40k)\", 'Medium Income': \"(\\$40k-\\$100k)\", 'High Income': \"($100k+)\"}\n",
    "fig, axs = plt.subplots(1, len(age_levels), figsize=(18, 6))  # 1 row, len(income_levels) columns\n",
    "\n",
    "for idx, level in enumerate(age_levels):\n",
    "    subset = age_party_pdf[age_party_pdf['Age_Level'] == level]\n",
    "    colors = [party_colors[party] for party in subset['Parties_Description']]\n",
    "    axs[idx].pie(subset['count'], labels=subset['Parties_Description'], \n",
    "                 autopct='%1.1f%%', startangle=140, colors=colors)\n",
    "    axs[idx].set_title(f'Age {level}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8597b2",
   "metadata": {},
   "source": [
    "# Model Building \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09d65f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged.drop('county_fips')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdb75bb",
   "metadata": {},
   "source": [
    "## Data Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064962af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting $ values to numeric AND Convert numeric columns stored as string to numeric\n",
    "\n",
    "# By Alex\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "\n",
    "# Convert string columns that contain \"$\" into numeric\n",
    "\n",
    "df_merged = df_merged.withColumn('CommercialData_EstHomeValue',\n",
    "                                 regexp_replace(col('CommercialData_EstHomeValue'), '[$,]', '').cast('float'))\n",
    "\n",
    "df_merged = df_merged.withColumn('CommercialData_EstimatedHHIncomeAmount',\n",
    "                                 regexp_replace(col('CommercialData_EstimatedHHIncomeAmount'), '[$,]', '').cast('float'))\n",
    "\n",
    "df_merged = df_merged.withColumn('CommercialData_EstimatedAreaMedianHHIncome',\n",
    "                                 regexp_replace(col('CommercialData_EstimatedAreaMedianHHIncome'), '[$,]', '').cast('float'))\n",
    "\n",
    "df_merged = df_merged.withColumn('CommercialData_AreaMedianHousingValue',\n",
    "                                 regexp_replace(col('CommercialData_AreaMedianHousingValue'), '[$,]', '').cast('float'))\n",
    "\n",
    "\n",
    "# Convert numeric columns stored as numeric into string\n",
    "\n",
    "# Columns to convert to numeric\n",
    "columns_to_convert = [\"Voters_Age\", \"Mailing_Families_HHCount\", \"Residence_Families_HHCount\", \"CommercialData_AreaMedianEducationYears\"]\n",
    "\n",
    "# Convert columns to numeric type\n",
    "for column in columns_to_convert:\n",
    "    df_merged = df_merged.withColumn(column, col(column).cast('float'))\n",
    "\n",
    "\n",
    "# PRINT RESULT\n",
    "\n",
    "test_rows = df_merged.head(10)\n",
    "\n",
    "column_headers= df_merged.columns\n",
    "\n",
    "new_df = pd.DataFrame(test_rows, columns=column_headers)\n",
    "\n",
    "new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2486d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c979e043",
   "metadata": {},
   "source": [
    "### Random Forest Model (IN JUPYTER NOTEBOOK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1481adbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the Model, Accuracy\n",
    "\n",
    "\n",
    "# TA CYRUS\n",
    "\n",
    "\n",
    "selectedColumns = [\n",
    "'Ethnic_Description', 'Voters_Gender', 'Voters_Age' , 'CommercialData_EstHomeValue','CommercialData_EstimatedHHIncomeAmount','CommercialData_EstimatedAreaMedianHHIncome','CommercialData_AreaMedianHousingValue', 'Parties_Description',\"Mailing_Families_HHCount\",\"Residence_Families_HHCount\", 'CommercialData_AreaMedianEducationYears', 'CommercialData_PropertyType', 'CommercialData_StateIncomeDecile', 'County','Mailing_HHGender_Description']   # FILL IN HERE ##########\n",
    " \n",
    "# Original:\n",
    "# selectedColumns = ['County', 'Mailing_Addresses_City', \n",
    "# 'Ethnic_Description', 'Voters_Gender', 'Voters_Age', \n",
    "# 'Mailing_Families_HHCount', 'Mailing_HHGender_Description', \n",
    "# 'Residence_Families_HHCount', 'CommercialData_EstHomeValue', \n",
    "# 'CommercialData_EstimatedHHIncomeAmount', \n",
    "# 'CommercialData_EstimatedAreaMedianHHIncome', \n",
    "# 'CommercialData_AreaMedianHousingValue', \n",
    "# 'CommercialData_AreaMedianEducationYears',\n",
    "# 'CommercialData_PropertyType', \n",
    "# 'CommercialData_StateIncomeDecile',\n",
    "# 'Parties_Description']\n",
    "\n",
    "categoricalColumns = ['County','Mailing_HHGender_Description',\"Ethnic_Description\",  \"Voters_Gender\", 'CommercialData_PropertyType','CommercialData_StateIncomeDecile']  # FILL IN HERE ################\n",
    "\n",
    "# categoricalColumns options \n",
    "# [\"County\", \"Mailing_Addresses_City\", \"Ethnic_Description\", \n",
    "# \"Voters_Gender\", \"Mailing_HHGender_Description\", \n",
    "# \"CommercialData_PropertyType\", \"CommercialData_StateIncomeDecile\"] \n",
    "\n",
    "columns_with_dollar_signs = ['CommercialData_EstHomeValue','CommercialData_EstimatedHHIncomeAmount','CommercialData_EstimatedAreaMedianHHIncome','CommercialData_AreaMedianHousingValue']   # FILL IN HERE  #####################\n",
    "\n",
    "# columns_with_dollar_signs options = [\n",
    "#     'CommercialData_EstHomeValue',\n",
    "#     'CommercialData_EstimatedHHIncomeAmount',\n",
    "#     'CommercialData_EstimatedAreaMedianHHIncome',\n",
    "#     'CommercialData_AreaMedianHousingValue'\n",
    "# ]\n",
    "\n",
    "numeric_columns_in_data_not_dollar_sign = [\"Voters_Age\",\"Mailing_Families_HHCount\",\"Residence_Families_HHCount\",'CommercialData_AreaMedianEducationYears']  # FILL IN HERE  #####\n",
    "\n",
    "# numeric_columns_in_data_not_dollar_sign options: [\"Voters_Age\", \n",
    "# \"Mailing_Families_HHCount\", \n",
    "# \"Residence_Families_HHCount\"]\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# columns_with_dollar_signs = [\n",
    "#     'CommercialData_EstHomeValue',\n",
    "#     'CommercialData_EstimatedHHIncomeAmount',\n",
    "#     'CommercialData_EstimatedAreaMedianHHIncome',\n",
    "#     'CommercialData_AreaMedianHousingValue'\n",
    "# ]\n",
    "\n",
    "for column in columns_with_dollar_signs:\n",
    "    df_merged = df_merged.withColumn(column,\n",
    "                                     regexp_replace(col(column), '[$,]', '').cast('float'))\n",
    "\n",
    "# numeric_columns_in_data_not_dollar_sign = [\"Voters_Age\", \"Mailing_Families_HHCount\", \"Residence_Families_HHCount\", \"CommercialData_AreaMedianEducationYears\"]\n",
    "\n",
    "# Convert columns to numeric type\n",
    "for column in numeric_columns_in_data_not_dollar_sign:\n",
    "    df_merged = df_merged.withColumn(column, col(column).cast('float'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# By Alex \n",
    "\n",
    "# TA Cyrus helped me find the following code and I used the following source:\n",
    "\n",
    "# https://stackoverflow.com/questions/35804755/apply-onehotencoder-for-several-categorical-columns-in-sparkmlib\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# categoricalColumns = [\"County\", \"Mailing_Addresses_City\", \"Ethnic_Description\", \"Voters_Gender\", \"Mailing_HHGender_Description\", \"CommercialData_PropertyType\", \"CommercialData_StateIncomeDecile\"]\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\", handleInvalid='keep') for column in categoricalColumns]\n",
    "\n",
    "\n",
    "# numeric_columns_in_data_not_dollar_sign = [\"Voters_Age\", \"Mailing_Families_HHCount\", \"Residence_Families_HHCount\", \"CommercialData_AreaMedianEducationYears\"]\n",
    "\n",
    "# one_hot_encoder_list = []\n",
    "# for indexer in indexers:\n",
    "#     ohe_col = \"{0}_encoded\".format(indexer.getOutputCol()) \n",
    "    \n",
    "#     one_hot_encoder = OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=ohe_col)\n",
    "#     one_hot_encoder_list.append(one_hot_encoder)\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[indexer.getOutputCol() for indexer in indexers] + numeric_columns_in_data_not_dollar_sign + columns_with_dollar_signs,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "\n",
    "(trainData, testData) = df_merged.randomSplit([0.7, 0.3], seed=123)\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"Parties_Description\", outputCol=\"label\")\n",
    "\n",
    "randomForest = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10,maxBins=100)\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + [labelIndexer, assembler, randomForest])\n",
    "\n",
    "\n",
    "\n",
    "# By Alex\n",
    "\n",
    "# FOR THE FOLLOWING CODE, YOU MUST RUN IT IN ON A CLUSTER ON GOOGLE COULD. \n",
    "# YOU WILL RUN OUT OF MEMORY RUNNING LOCALLY (java.lang.OutOfMemoryError: Java heap space)\n",
    "\n",
    "model = pipeline.fit(trainData)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time in seconds\n",
    "elapsed_time_seconds = end_time - start_time\n",
    "\n",
    "# Convert elapsed time to minutes\n",
    "elapsed_time_minutes = elapsed_time_seconds / 60\n",
    "\n",
    "print(\"Time taken:\", elapsed_time_minutes, \"minutes\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "import time\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "predictions = model.transform(testData)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time in seconds\n",
    "elapsed_time_seconds = end_time - start_time\n",
    "\n",
    "# Convert elapsed time to minutes\n",
    "elapsed_time_minutes = elapsed_time_seconds / 60\n",
    "\n",
    "print(\"Time taken:\", elapsed_time_minutes, \"minutes\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ddec47",
   "metadata": {},
   "source": [
    "Output\n",
    "\n",
    "Time taken: 7.387498692671458 minutes\n",
    "\n",
    "\n",
    "Accuracy: 0.459\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f4afdd",
   "metadata": {},
   "source": [
    "#### Feature Importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bde7df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "rf_model = model.stages[-1] \n",
    "\n",
    "feature_importance = rf_model.featureImportances.toArray()\n",
    "\n",
    "feature_cols = model.stages[-2].getInputCols()\n",
    "\n",
    "\n",
    "feature_df = pd.DataFrame({\"Feature\": feature_cols, \"Importance\": feature_importance})\n",
    "\n",
    "feature_df = feature_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Plotting the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_df[\"Feature\"], feature_df[\"Importance\"], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the highest importance at the top\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71463e49",
   "metadata": {},
   "source": [
    "### Multinomial Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d766e07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the Model, Accuracy\n",
    "# Axl and Alex\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "\n",
    "# categoricalColumns = [\"County\", \"Mailing_Addresses_City\", \"Ethnic_Description\", \"Voters_Gender\", \"Mailing_HHGender_Description\", \"CommercialData_PropertyType\", \"CommercialData_StateIncomeDecile\"]\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\", handleInvalid='keep') for column in categoricalColumns if column != 'Parties_Description']\n",
    "\n",
    "\n",
    "# numeric_columns_in_data_not_dollar_sign = [\"Voters_Age\", \"Mailing_Families_HHCount\", \"Residence_Families_HHCount\", \"CommercialData_AreaMedianEducationYears\"]\n",
    "\n",
    "one_hot_encoder_list = []\n",
    "for indexer in indexers:\n",
    "    ohe_col = \"{0}_encoded\".format(indexer.getOutputCol()) \n",
    "    \n",
    "    one_hot_encoder = OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=ohe_col)\n",
    "    one_hot_encoder_list.append(one_hot_encoder)\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[indexer.getOutputCol() for indexer in indexers] + numeric_columns_in_data_not_dollar_sign + columns_with_dollar_signs,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "\n",
    "featureArr = [index.getOutputCol() for index in indexers] + [('scaled_' + f) for f in numeric_columns_in_data_not_dollar_sign + columns_with_dollar_signs]\n",
    "\n",
    "va1 = [VectorAssembler(inputCols = [f], outputCol=('vec_' + f)) for f in numeric_columns_in_data_not_dollar_sign + columns_with_dollar_signs]\n",
    "ss = [StandardScaler(inputCol=\"vec_\" + f, outputCol=\"scaled_\" + f, withMean=True, withStd=True) for f in numeric_columns_in_data_not_dollar_sign + columns_with_dollar_signs]\n",
    "\n",
    "va2 = VectorAssembler(inputCols=featureArr, outputCol=\"features\")\n",
    "lr = LogisticRegression()\n",
    "\n",
    "\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"Parties_Description\", outputCol=\"label\")\n",
    "\n",
    "\n",
    "stages = va1 + ss + indexers + [va2, labelIndexer, lr]\n",
    "\n",
    "log_pipeline = Pipeline(stages=stages)\n",
    "\n",
    "(trainData, testData) = df_merged.randomSplit([0.7, 0.3], seed=123)\n",
    "\n",
    "log_model = log_pipeline.fit(trainData)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# log_reg = LogisticRegression(labelCol=\"label\", featuresCol=\"scaled_features\")\n",
    "\n",
    "# log_pipeline = Pipeline(stages=indexers + [scaler, labelIndexer, assembler, log_reg])\n",
    "\n",
    "# log_model = log_pipeline.fit(trainData)\n",
    "\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time in seconds\n",
    "elapsed_time_seconds = end_time - start_time\n",
    "\n",
    "# Convert elapsed time to minutes\n",
    "elapsed_time_minutes = elapsed_time_seconds / 60\n",
    "\n",
    "print(\"Time taken:\", elapsed_time_minutes, \"minutes\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "import time\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "predictions = log_model.transform(testData)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time in seconds\n",
    "elapsed_time_seconds = end_time - start_time\n",
    "\n",
    "# Convert elapsed time to minutes\n",
    "elapsed_time_minutes = elapsed_time_seconds / 60\n",
    "\n",
    "print(\"Time taken:\", elapsed_time_minutes, \"minutes\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a9030a",
   "metadata": {},
   "source": [
    "Output\n",
    "\n",
    "Time taken: 5.091986711819967 minutes\n",
    "\n",
    "Accuracy: 0.46134799364665036\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c00e25b",
   "metadata": {},
   "source": [
    "#### Feature Importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df6c620",
   "metadata": {},
   "outputs": [],
   "source": [
    "orderedPartyList = ['Democratic','Non-Partisan','Republican','Other']  # This order comes from TA Cyrus. It is in this order because apparently it goes descending by count for each party in df_merged.  -Cyrus\n",
    "\n",
    "\n",
    "coefficients = log_model.stages[-1].coefficientMatrix.toArray()\n",
    "\n",
    "feature_names = log_model.stages[-3].getInputCols()\n",
    "\n",
    "\n",
    "# Create 4 bar plots\n",
    "for i in range(4):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(feature_names, coefficients[i])\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Coefficient Value')\n",
    "    plt.title('Coefficients for {}'.format(orderedPartyList[i]))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e457a52",
   "metadata": {},
   "source": [
    "#### Getting the top input for each of the top 5 predictors given a party preference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9886b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axl - calculate the absolute coefficients\n",
    "coefficients_df = pd.DataFrame(coefficients, columns=feature_names, index=orderedPartyList)\n",
    "\n",
    "# Get the top 5 predictors for each party preference\n",
    "top_predictors = {}\n",
    "for party in orderedPartyList:\n",
    "    top_predictors[party] = coefficients_df.loc[party].nlargest(5)\n",
    "\n",
    "# Print the top 5 predictors for each party preference\n",
    "for party, predictors in top_predictors.items():\n",
    "    print(f\"Top 5 predictors for {party}:\")\n",
    "    print(predictors)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679a3bb2",
   "metadata": {},
   "source": [
    "#### Finding the top input for each predictor within each party group (standardized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0172a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axl - find the top input for each predictor within each party group (standardized)\n",
    "def find_top_inputs(predictions, top_predictors):\n",
    "    top_inputs = {}\n",
    "\n",
    "    # Iterate over each party preference\n",
    "    for party, predictors in top_predictors.items():\n",
    "        top_inputs[party] = {}\n",
    "\n",
    "        # Filter predictions DataFrame for the current party\n",
    "        party_df = predictions.filter(predictions[\"Parties_Description\"] == party)\n",
    "\n",
    "        # Iterate over the top predictors for the current party\n",
    "        for predictor in predictors.index:\n",
    "            # Group by the predictor and count occurrences of each input value\n",
    "            grouped_df = party_df.groupBy(predictor).count()\n",
    "\n",
    "            # Order by count in descending order\n",
    "            ordered_df = grouped_df.orderBy(\"count\", ascending=False)\n",
    "\n",
    "            # Get the top input for the predictor\n",
    "            top_input = ordered_df.first()[predictor]\n",
    "\n",
    "            # Store the top input for the predictor\n",
    "            top_inputs[party][predictor] = top_input\n",
    "\n",
    "    return top_inputs\n",
    "\n",
    "# Call the function to find the top inputs for each party preference\n",
    "top_inputs = find_top_inputs(predictions, top_predictors)\n",
    "\n",
    "# Print the top inputs for each party preference and predictor\n",
    "for party, predictors in top_inputs.items():\n",
    "    print(f\"Top inputs for {party}:\")\n",
    "    for predictor, top_input in predictors.items():\n",
    "        print(f\"{predictor}: {top_input}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff483d2",
   "metadata": {},
   "source": [
    "#### Finding the top input for each predictor within each party group (True values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2544f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axl - top 5 predictors for each party preference (unscaled and not indexed)\n",
    "top_5_unscaled_predictors_Democratic = ['Voters_Gender', 'CommercialData_AreaMedianEducationYears', 'Voters_Age', 'CommercialData_EstimatedAreaMedianHHIncome', 'Residence_Families_HHCount']\n",
    "top_5_unscaled_predictors_Non_Partisan = ['Voters_Age', 'Voters_Gender', 'Mailing_HHGender_Description', 'CommercialData_PropertyType', 'Residence_Families_HHCount']\n",
    "top_5_unscaled_predictors_Republican = ['Voters_Age', 'CommercialData_AreaMedianEducationYears', 'Mailing_HHGender_Description', 'CommercialData_EstimatedAreaMedianHHIncome', 'Residence_Families_HHCount']\n",
    "top_5_unscaled_predictors_Other = ['Voters_Gender', 'Voters_Age', 'CommercialData_EstimatedAreaMedianHHIncome', 'CommercialData_AreaMedianHousingValue', 'CommercialData_PropertyType']\n",
    "\n",
    "# Axl - find the top input for each predictor within each party group (standardized)\n",
    "def find_top_inputs(predictions, top_predictors):\n",
    "    top_inputs = {}\n",
    "\n",
    "    # Iterate over each party preference\n",
    "    for party, predictors in top_predictors.items():\n",
    "        top_inputs[party] = {}\n",
    "\n",
    "        # Filter predictions DataFrame for the current party\n",
    "        party_df = predictions.filter(predictions[\"Parties_Description\"] == party)\n",
    "\n",
    "        # Iterate over the top predictors for the current party\n",
    "        for predictor in predictors:\n",
    "            # Group by the predictor and count occurrences of each input value\n",
    "            grouped_df = party_df.groupBy(predictor).count()\n",
    "\n",
    "            # Order by count in descending order\n",
    "            ordered_df = grouped_df.orderBy(\"count\", ascending=False)\n",
    "\n",
    "            # Get the top input for the predictor\n",
    "            top_input = ordered_df.first()[predictor]\n",
    "\n",
    "            # Store the top input for the predictor\n",
    "            top_inputs[party][predictor] = top_input\n",
    "\n",
    "    return top_inputs\n",
    "\n",
    "# Define top predictors for each party preference\n",
    "top_predictors = {\n",
    "    'Democratic': top_5_unscaled_predictors_Democratic,\n",
    "    'Non-Partisan': top_5_unscaled_predictors_Non_Partisan,\n",
    "    'Republican': top_5_unscaled_predictors_Republican,\n",
    "    'Other': top_5_unscaled_predictors_Other\n",
    "}\n",
    "\n",
    "# Call the function to find the top inputs for each party preference\n",
    "top_inputs = find_top_inputs(predictions, top_predictors)\n",
    "\n",
    "# Print the top inputs for each party preference and predictor\n",
    "for party, predictors in top_inputs.items():\n",
    "    print(f\"Top inputs for {party}:\")\n",
    "    for predictor, top_input in predictors.items():\n",
    "        print(f\"{predictor}: {top_input}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e0ccb9",
   "metadata": {},
   "source": [
    "## Generalizability (Running on Washington Data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2e4143",
   "metadata": {},
   "source": [
    "### Setting up the cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20d2b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written by Alex Roginski\n",
    "\n",
    "# 1. Create a project\n",
    "\n",
    "# 2. Go to Cloud Storage --> Buckets and create one\n",
    "\n",
    "# Now, run this code in the Cloud Shell:\n",
    "export PROJECT_ID=\"\" ## Your project ID here. NO LEADING OR TRAILING SPACES AROUND \"=\".\n",
    "\n",
    "export BUCKET_NAME=\"\" ## Your Bucket Name that you just created\n",
    "\n",
    "export CLUSTER_NAME=\"\" # Make your own cluster name here (whatever you want)\n",
    "\n",
    "# The following comes from the project assignment pdf. Run line by line.\n",
    "\n",
    "gsutil -u $PROJECT_ID ls gs://winter-2024-voter-file/\n",
    "\n",
    "# gsutil -u $PROJECT_ID -m cp -r gs://winter-2024-voter-file/VM2Uniform/VM2Uniform--WY--2021-01-13 gs://$BUCKET_NAME\n",
    "\n",
    "\n",
    "gcloud dataproc clusters create $CLUSTER_NAME  --enable-component-gateway  --bucket $BUCKET_NAME  --region us-central1  --single-node  --master-machine-type e2-standard-4  --master-boot-disk-size 500  --image-version 2.1-debian11  --optional-components JUPYTER  --project $PROJECT_ID\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76579686",
   "metadata": {},
   "source": [
    "### Setup OREGON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96785444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Jupyter notebook hosted by Google Cloud\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Voter File Data\") \\\n",
    ".config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = (\n",
    "spark.read.format(\"parquet\")  # Edit by Alex\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(\"gs://winter-2024-voter-file/VM2Uniform/VM2Uniform--OR--2021-02-05\")\n",
    ")\n",
    "# By Andy\n",
    "# This will load the uscounties.csv file into df2 variable, which will contain County IDs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# By Alex\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "selectedColumns = ['County', 'Ethnic_Description', 'Voters_Gender', 'Voters_Age', 'Mailing_Families_HHCount', 'Mailing_HHGender_Description', 'Residence_Families_HHCount', 'CommercialData_EstHomeValue', 'CommercialData_EstimatedHHIncomeAmount', 'CommercialData_EstimatedAreaMedianHHIncome', 'CommercialData_AreaMedianHousingValue', 'CommercialData_AreaMedianEducationYears', 'CommercialData_PropertyType', 'CommercialData_StateIncomeDecile','Parties_Description']\n",
    "\n",
    "cleaned_df = df.select(selectedColumns)\n",
    "\n",
    "data = cleaned_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "df_merged = data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Andy's code to merge bin\n",
    "from pyspark.sql.functions import when, col, lit\n",
    "\n",
    "parties_to_keep = ['Republican', 'Democratic', 'Non-Partisan']  # List of parties to keep\n",
    "\n",
    "df_merged = df_merged.withColumn(\"Parties_Description\", when(col(\"Parties_Description\").isin(parties_to_keep), col(\"Parties_Description\")).otherwise(lit(\"Other\")))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# By Alex\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "\n",
    "# Convert string columns that contain \"$\" into numeric\n",
    "\n",
    "df_merged = df_merged.withColumn('CommercialData_EstHomeValue',\n",
    "                                 regexp_replace(col('CommercialData_EstHomeValue'), '[$,]', '').cast('float'))\n",
    "\n",
    "df_merged = df_merged.withColumn('CommercialData_EstimatedHHIncomeAmount',\n",
    "                                 regexp_replace(col('CommercialData_EstimatedHHIncomeAmount'), '[$,]', '').cast('float'))\n",
    "\n",
    "df_merged = df_merged.withColumn('CommercialData_EstimatedAreaMedianHHIncome',\n",
    "                                 regexp_replace(col('CommercialData_EstimatedAreaMedianHHIncome'), '[$,]', '').cast('float'))\n",
    "\n",
    "df_merged = df_merged.withColumn('CommercialData_AreaMedianHousingValue',\n",
    "                                 regexp_replace(col('CommercialData_AreaMedianHousingValue'), '[$,]', '').cast('float'))\n",
    "\n",
    "\n",
    "# Convert numeric columns stored as numeric into string\n",
    "\n",
    "# Columns to convert to numeric\n",
    "columns_to_convert = [\"Voters_Age\", \"Mailing_Families_HHCount\", \"Residence_Families_HHCount\", \"CommercialData_AreaMedianEducationYears\"]\n",
    "\n",
    "# Convert columns to numeric type\n",
    "for column in columns_to_convert:\n",
    "    df_merged = df_merged.withColumn(column, col(column).cast('float'))\n",
    "\n",
    "\n",
    "# PRINT RESULT\n",
    "\n",
    "test_rows = df_merged.head(10)\n",
    "\n",
    "column_headers= df_merged.columns\n",
    "\n",
    "new_df = pd.DataFrame(test_rows, columns=column_headers)\n",
    "\n",
    "new_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b4c030",
   "metadata": {},
   "source": [
    "#### Random Forest setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1852dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TA CYRUS\n",
    "\n",
    "\n",
    "selectedColumns = [\n",
    "'Ethnic_Description', 'Voters_Gender', 'Voters_Age' , 'CommercialData_EstHomeValue','CommercialData_EstimatedHHIncomeAmount','CommercialData_EstimatedAreaMedianHHIncome','CommercialData_AreaMedianHousingValue', 'Parties_Description',\"Mailing_Families_HHCount\",\"Residence_Families_HHCount\", 'CommercialData_AreaMedianEducationYears', 'CommercialData_PropertyType', 'CommercialData_StateIncomeDecile', 'County','Mailing_HHGender_Description']   # FILL IN HERE ##########\n",
    " \n",
    "# Original:\n",
    "# selectedColumns = ['County', 'Mailing_Addresses_City', \n",
    "# 'Ethnic_Description', 'Voters_Gender', 'Voters_Age', \n",
    "# 'Mailing_Families_HHCount', 'Mailing_HHGender_Description', \n",
    "# 'Residence_Families_HHCount', 'CommercialData_EstHomeValue', \n",
    "# 'CommercialData_EstimatedHHIncomeAmount', \n",
    "# 'CommercialData_EstimatedAreaMedianHHIncome', \n",
    "# 'CommercialData_AreaMedianHousingValue', \n",
    "# 'CommercialData_AreaMedianEducationYears',\n",
    "# 'CommercialData_PropertyType', \n",
    "# 'CommercialData_StateIncomeDecile',\n",
    "# 'Parties_Description']\n",
    "\n",
    "categoricalColumns = ['County','Mailing_HHGender_Description',\"Ethnic_Description\",  \"Voters_Gender\", 'CommercialData_PropertyType','CommercialData_StateIncomeDecile']  # FILL IN HERE ################\n",
    "\n",
    "# categoricalColumns options \n",
    "# [\"County\", \"Mailing_Addresses_City\", \"Ethnic_Description\", \n",
    "# \"Voters_Gender\", \"Mailing_HHGender_Description\", \n",
    "# \"CommercialData_PropertyType\", \"CommercialData_StateIncomeDecile\"] \n",
    "\n",
    "columns_with_dollar_signs = ['CommercialData_EstHomeValue','CommercialData_EstimatedHHIncomeAmount','CommercialData_EstimatedAreaMedianHHIncome','CommercialData_AreaMedianHousingValue']   # FILL IN HERE  #####################\n",
    "\n",
    "# columns_with_dollar_signs options = [\n",
    "#     'CommercialData_EstHomeValue',\n",
    "#     'CommercialData_EstimatedHHIncomeAmount',\n",
    "#     'CommercialData_EstimatedAreaMedianHHIncome',\n",
    "#     'CommercialData_AreaMedianHousingValue'\n",
    "# ]\n",
    "\n",
    "numeric_columns_in_data_not_dollar_sign = [\"Voters_Age\",\"Mailing_Families_HHCount\",\"Residence_Families_HHCount\",'CommercialData_AreaMedianEducationYears']  # FILL IN HERE  #####\n",
    "\n",
    "# numeric_columns_in_data_not_dollar_sign options: [\"Voters_Age\", \n",
    "# \"Mailing_Families_HHCount\", \n",
    "# \"Residence_Families_HHCount\"]\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# columns_with_dollar_signs = [\n",
    "#     'CommercialData_EstHomeValue',\n",
    "#     'CommercialData_EstimatedHHIncomeAmount',\n",
    "#     'CommercialData_EstimatedAreaMedianHHIncome',\n",
    "#     'CommercialData_AreaMedianHousingValue'\n",
    "# ]\n",
    "\n",
    "for column in columns_with_dollar_signs:\n",
    "    df_merged = df_merged.withColumn(column,\n",
    "                                     regexp_replace(col(column), '[$,]', '').cast('float'))\n",
    "\n",
    "# numeric_columns_in_data_not_dollar_sign = [\"Voters_Age\", \"Mailing_Families_HHCount\", \"Residence_Families_HHCount\", \"CommercialData_AreaMedianEducationYears\"]\n",
    "\n",
    "# Convert columns to numeric type\n",
    "for column in numeric_columns_in_data_not_dollar_sign:\n",
    "    df_merged = df_merged.withColumn(column, col(column).cast('float'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# By Alex \n",
    "\n",
    "# TA Cyrus helped me find the following code and I used the following source:\n",
    "\n",
    "# https://stackoverflow.com/questions/35804755/apply-onehotencoder-for-several-categorical-columns-in-sparkmlib\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# categoricalColumns = [\"County\", \"Mailing_Addresses_City\", \"Ethnic_Description\", \"Voters_Gender\", \"Mailing_HHGender_Description\", \"CommercialData_PropertyType\", \"CommercialData_StateIncomeDecile\"]\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\", handleInvalid='keep') for column in categoricalColumns]\n",
    "\n",
    "\n",
    "# numeric_columns_in_data_not_dollar_sign = [\"Voters_Age\", \"Mailing_Families_HHCount\", \"Residence_Families_HHCount\", \"CommercialData_AreaMedianEducationYears\"]\n",
    "\n",
    "# one_hot_encoder_list = []\n",
    "# for indexer in indexers:\n",
    "#     ohe_col = \"{0}_encoded\".format(indexer.getOutputCol()) \n",
    "    \n",
    "#     one_hot_encoder = OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=ohe_col)\n",
    "#     one_hot_encoder_list.append(one_hot_encoder)\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[indexer.getOutputCol() for indexer in indexers] + numeric_columns_in_data_not_dollar_sign + columns_with_dollar_signs,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "\n",
    "(trainData, testData) = df_merged.randomSplit([0.7, 0.3], seed=123)\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"Parties_Description\", outputCol=\"label\")\n",
    "\n",
    "randomForest = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10,maxBins=100)\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + [labelIndexer, assembler, randomForest])\n",
    "\n",
    "\n",
    "\n",
    "# By Alex\n",
    "\n",
    "# FOR THE FOLLOWING CODE, YOU MUST RUN IT IN ON A CLUSTER ON GOOGLE COULD. \n",
    "# YOU WILL RUN OUT OF MEMORY RUNNING LOCALLY (java.lang.OutOfMemoryError: Java heap space)\n",
    "\n",
    "model = pipeline.fit(trainData)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time in seconds\n",
    "elapsed_time_seconds = end_time - start_time\n",
    "\n",
    "# Convert elapsed time to minutes\n",
    "elapsed_time_minutes = elapsed_time_seconds / 60\n",
    "\n",
    "print(\"Time taken:\", elapsed_time_minutes, \"minutes\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "import time\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "predictions = model.transform(testData)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time in seconds\n",
    "elapsed_time_seconds = end_time - start_time\n",
    "\n",
    "# Convert elapsed time to minutes\n",
    "elapsed_time_minutes = elapsed_time_seconds / 60\n",
    "\n",
    "print(\"Time taken:\", elapsed_time_minutes, \"minutes\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33911ece",
   "metadata": {},
   "source": [
    "### Setup WASHINGTON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e354ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Jupyter notebook hosted by Google Cloud\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Voter File Data\") \\\n",
    ".config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "DATA_DIRECTORY = \"gs://winter-2024-voter-file/VM2Uniform/VM2Uniform--WA--2020-12-09/\"\n",
    "\n",
    "\n",
    "df = (\n",
    "spark.read.format(\"parquet\")  # Edit by Alex\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(DATA_DIRECTORY)\n",
    ")\n",
    "# By Andy\n",
    "# This will load the uscounties.csv file into df2 variable, which will contain County IDs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# By Alex\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "selectedColumns = ['County', 'Ethnic_Description', 'Voters_Gender', 'Voters_Age', 'Mailing_Families_HHCount', 'Mailing_HHGender_Description', 'Residence_Families_HHCount', 'CommercialData_EstHomeValue', 'CommercialData_EstimatedHHIncomeAmount', 'CommercialData_EstimatedAreaMedianHHIncome', 'CommercialData_AreaMedianHousingValue', 'CommercialData_AreaMedianEducationYears', 'CommercialData_PropertyType', 'CommercialData_StateIncomeDecile','Parties_Description']\n",
    "\n",
    "cleaned_df = df.select(selectedColumns)\n",
    "\n",
    "data = cleaned_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "df_merged = data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Andy's code to merge bin\n",
    "from pyspark.sql.functions import when, col, lit\n",
    "\n",
    "parties_to_keep = ['Republican', 'Democratic', 'Non-Partisan']  # List of parties to keep\n",
    "\n",
    "df_merged = df_merged.withColumn(\"Parties_Description\", when(col(\"Parties_Description\").isin(parties_to_keep), col(\"Parties_Description\")).otherwise(lit(\"Other\")))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# By Alex\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "\n",
    "# Convert string columns that contain \"$\" into numeric\n",
    "\n",
    "df_merged = df_merged.withColumn('CommercialData_EstHomeValue',\n",
    "                                 regexp_replace(col('CommercialData_EstHomeValue'), '[$,]', '').cast('float'))\n",
    "\n",
    "df_merged = df_merged.withColumn('CommercialData_EstimatedHHIncomeAmount',\n",
    "                                 regexp_replace(col('CommercialData_EstimatedHHIncomeAmount'), '[$,]', '').cast('float'))\n",
    "\n",
    "df_merged = df_merged.withColumn('CommercialData_EstimatedAreaMedianHHIncome',\n",
    "                                 regexp_replace(col('CommercialData_EstimatedAreaMedianHHIncome'), '[$,]', '').cast('float'))\n",
    "\n",
    "df_merged = df_merged.withColumn('CommercialData_AreaMedianHousingValue',\n",
    "                                 regexp_replace(col('CommercialData_AreaMedianHousingValue'), '[$,]', '').cast('float'))\n",
    "\n",
    "\n",
    "# Convert numeric columns stored as numeric into string\n",
    "\n",
    "# Columns to convert to numeric\n",
    "columns_to_convert = [\"Voters_Age\", \"Mailing_Families_HHCount\", \"Residence_Families_HHCount\", \"CommercialData_AreaMedianEducationYears\"]\n",
    "\n",
    "# Convert columns to numeric type\n",
    "for column in columns_to_convert:\n",
    "    df_merged = df_merged.withColumn(column, col(column).cast('float'))\n",
    "\n",
    "\n",
    "# PRINT RESULT\n",
    "\n",
    "test_rows = df_merged.head(10)\n",
    "\n",
    "column_headers= df_merged.columns\n",
    "\n",
    "new_df = pd.DataFrame(test_rows, columns=column_headers)\n",
    "\n",
    "new_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96931769",
   "metadata": {},
   "source": [
    "### Random Forest Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef5f74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = df_merged\n",
    "\n",
    "predictions = model.transform(testData)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dcc9b9",
   "metadata": {},
   "source": [
    "Accuracy: 0.422 for Washington\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc83e633",
   "metadata": {},
   "source": [
    "### Logistic Regression Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4074677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = log_model.transform(testData)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8026141f",
   "metadata": {},
   "source": [
    "Accuracy: 0.436\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9027d18",
   "metadata": {},
   "source": [
    "### Setup MAINE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61873d8",
   "metadata": {},
   "source": [
    "#### Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731ab8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Jupyter notebook hosted by Google Cloud\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read Voter File Data\") \\\n",
    ".config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "DATA_DIRECTORY = \"gs://winter-2024-voter-file/VM2Uniform/VM2Uniform--WA--2020-12-09/\"\n",
    "\n",
    "\n",
    "df = (\n",
    "spark.read.format(\"parquet\")  # Edit by Alex\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(DATA_DIRECTORY)\n",
    ")\n",
    "# By Andy\n",
    "# This will load the uscounties.csv file into df2 variable, which will contain County IDs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# By Alex\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "selectedColumns = ['County', 'Ethnic_Description', 'Voters_Gender', 'Voters_Age', 'Mailing_Families_HHCount', 'Mailing_HHGender_Description', 'Residence_Families_HHCount', 'CommercialData_EstHomeValue', 'CommercialData_EstimatedHHIncomeAmount', 'CommercialData_EstimatedAreaMedianHHIncome', 'CommercialData_AreaMedianHousingValue', 'CommercialData_AreaMedianEducationYears', 'CommercialData_PropertyType', 'CommercialData_StateIncomeDecile','Parties_Description']\n",
    "\n",
    "cleaned_df = df.select(selectedColumns)\n",
    "\n",
    "data = cleaned_df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "df_merged = data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Andy's code to merge bin\n",
    "from pyspark.sql.functions import when, col, lit\n",
    "\n",
    "parties_to_keep = ['Republican', 'Democratic', 'Non-Partisan']  # List of parties to keep\n",
    "\n",
    "df_merged = df_merged.withColumn(\"Parties_Description\", when(col(\"Parties_Description\").isin(parties_to_keep), col(\"Parties_Description\")).otherwise(lit(\"Other\")))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# By Alex\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "\n",
    "# Convert string columns that contain \"$\" into numeric\n",
    "\n",
    "df_merged = df_merged.withColumn('CommercialData_EstHomeValue',\n",
    "                                 regexp_replace(col('CommercialData_EstHomeValue'), '[$,]', '').cast('float'))\n",
    "\n",
    "df_merged = df_merged.withColumn('CommercialData_EstimatedHHIncomeAmount',\n",
    "                                 regexp_replace(col('CommercialData_EstimatedHHIncomeAmount'), '[$,]', '').cast('float'))\n",
    "\n",
    "df_merged = df_merged.withColumn('CommercialData_EstimatedAreaMedianHHIncome',\n",
    "                                 regexp_replace(col('CommercialData_EstimatedAreaMedianHHIncome'), '[$,]', '').cast('float'))\n",
    "\n",
    "df_merged = df_merged.withColumn('CommercialData_AreaMedianHousingValue',\n",
    "                                 regexp_replace(col('CommercialData_AreaMedianHousingValue'), '[$,]', '').cast('float'))\n",
    "\n",
    "\n",
    "# Convert numeric columns stored as numeric into string\n",
    "\n",
    "# Columns to convert to numeric\n",
    "columns_to_convert = [\"Voters_Age\", \"Mailing_Families_HHCount\", \"Residence_Families_HHCount\", \"CommercialData_AreaMedianEducationYears\"]\n",
    "\n",
    "# Convert columns to numeric type\n",
    "for column in columns_to_convert:\n",
    "    df_merged = df_merged.withColumn(column, col(column).cast('float'))\n",
    "\n",
    "\n",
    "# PRINT RESULT\n",
    "\n",
    "test_rows = df_merged.head(10)\n",
    "\n",
    "column_headers= df_merged.columns\n",
    "\n",
    "new_df = pd.DataFrame(test_rows, columns=column_headers)\n",
    "\n",
    "new_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43be342",
   "metadata": {},
   "source": [
    "Accuracy: 0.384"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daad55a1",
   "metadata": {},
   "source": [
    "### Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25882b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = df_merged\n",
    "predictions = log_model.transform(testData)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4e8ddb",
   "metadata": {},
   "source": [
    "Accuracy: 0.393"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecad594a",
   "metadata": {},
   "source": [
    "### setup LOUISIANA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8147caed",
   "metadata": {},
   "source": [
    "Random Forest\n",
    "\n",
    "Accuracy: 0.361"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfd3bef",
   "metadata": {},
   "source": [
    "#### Logistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612b5acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = df_merged\n",
    "predictions = log_model.transform(testData)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a155b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
